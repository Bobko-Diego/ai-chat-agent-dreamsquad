import os
import asyncio
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from strands import Agent
from strands.models.ollama import OllamaModel
from tools.calculator import calculator

load_dotenv()

OLLAMA_HOST = os.getenv("OLLAMA_BASE_URL")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL")
API_HOST = os.getenv("API_HOST")
API_PORT = int(os.getenv("API_PORT"))

app = FastAPI(
    title="AI Chat Agent Dreamsquad",
    description="API de Chat com Agente de IA da Dreamsquad capaz de realizar c√°lculos matem√°ticos",
    version="1.0.0"
)

class ChatRequest(BaseModel):
    """Esse √© o modelo para requisi√ß√£o de chat"""
    message: str = Field(..., description="Mensagem do usu√°rio", min_length=1)
    
    class Config:
        json_schema_extra = {
            "example": {
                "message": "Quanto √© 1234 * 5678?"
            }
        }


class ChatResponse(BaseModel):
    """Esse √© o modelo para resposta de chat"""
    response: str = Field(..., description="Resposta do agente de IA")
    
    class Config:
        json_schema_extra = {
            "example": {
                "response": "O resultado de 1234 * 5678 √© 7006652."
            }
        }


def get_event_loop():
    try:
        loop = asyncio.get_event_loop()
        if loop.is_closed():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        return loop
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        return loop


def create_agent():
    model = OllamaModel(
        host=OLLAMA_HOST,
        model_id=OLLAMA_MODEL,
        temperature=0.7
    )
    
    system_prompt = """Voc√™ √© um assistente √∫til e amig√°vel que pode responder perguntas de conhecimento geral em portugu√™s.

    Voc√™ tem acesso a uma ferramenta de calculadora, mas deve us√°-la APENAS quando o usu√°rio fizer uma pergunta que claramente requer um c√°lculo matem√°tico.

    QUANDO USAR A FERRAMENTA CALCULATOR:
    - Perguntas expl√≠citas de c√°lculo: "Quanto √© 1234 * 5678?"
    - Opera√ß√µes matem√°ticas: "Calcule 2 elevado a 10"
    - Ra√≠zes e fun√ß√µes: "Qual a raiz quadrada de 144?"
    - Express√µes num√©ricas: "Quanto √© (15 + 25) / 2?"

    QUANDO N√ÉO USAR A FERRAMENTA (responda diretamente):
    - Cumprimentos: "Ol√°!", "Oi!", "Tudo bem?"
    - Perguntas gerais: "O que √© intelig√™ncia artificial?"
    - Conversas casuais: "Como voc√™ est√°?", "Qual seu nome?"
    - Conhecimento geral: "Quem foi Einstein?", "O que √© FastAPI?"

    Para perguntas que requerem c√°lculos b√°sicos, c√°lculos complexos ou que exigem precis√£o, use a ferramenta calculator.
    Para todo o resto, responda diretamente de forma amig√°vel e informativa."""
    
    agent = Agent(
        model=model,
        tools=[calculator],
        system_prompt=system_prompt,
        state=None 
    )
    
    return agent


@app.on_event("startup")
async def startup_event():
    try:
        test_agent = create_agent()
        print(f"‚úÖ Agente inicializado com sucesso!")
        print(f"üì¶ Modelo: {OLLAMA_MODEL}")
        print(f"üåê Host Ollama: {OLLAMA_HOST}")
        print(f"üîß Tools dispon√≠veis: {test_agent.tool_names}")
    except Exception as e:
        print(f"‚ùå Erro ao inicializar o agente: {e}")
        print(f"‚ö†Ô∏è  Verifique se o Ollama est√° rodando: ollama serve")


@app.get("/")
async def root():
    """Esse √© o endpoint raiz com informa√ß√µes da API"""
    return {
        "message": "Chat Agent API est√° funcionando! ‚úÖ",
        "endpoints": {
            "chat": "/chat",
            "health": "/health",
            "docs": "/docs"
        },
        "model": OLLAMA_MODEL,
        "ollama_host": OLLAMA_HOST,
        "note": "Cada requisi√ß√£o √© independente (sem mem√≥ria entre chamadas)"
    }


@app.get("/health")
async def health_check():
    try:
        test_agent = create_agent()
        return {
            "status": "healthy",
            "model": OLLAMA_MODEL,
            "ollama_host": OLLAMA_HOST,
            "tools_loaded": len(test_agent.tool_names),
            "tools": test_agent.tool_names
        }
    except Exception as e:
        raise HTTPException(
            status_code=503,
            detail=f"Servi√ßo indispon√≠vel: {str(e)}"
        )


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Endpoint para processar a mensagem dos usu√°rios
    """
    try:
        loop = get_event_loop()
        
        agent = create_agent()
        
        def run_agent():
            response = agent(request.message)
            return response
        
        response = await loop.run_in_executor(None, run_agent)
        
        if isinstance(response, str):
            agent_response = response
        elif hasattr(response, 'content'):

            if isinstance(response.content, list):

                text_parts = [
                    item.get('text', '') 
                    for item in response.content 
                    if isinstance(item, dict) and 'text' in item
                ]
                agent_response = '\n'.join(text_parts) if text_parts else str(response.content)
            else:
                agent_response = response.content
        else:
            agent_response = str(response)
        
        return ChatResponse(response=agent_response.strip())
    
    except Exception as e:

        print(f"‚ùå Erro ao processar mensagem '{request.message}': {str(e)}")
        import traceback
        traceback.print_exc()
        
        raise HTTPException(
            status_code=500,
            detail=f"Erro ao processar a mensagem: {str(e)}"
        )


if __name__ == "__main__":
    import uvicorn
    
    print("\n" + "="*50)
    print("üöÄ Iniciando Chat Agent API")
    print("="*50)
    print(f"üìç URL: http://{API_HOST}:{API_PORT}")
    print(f"üìö Documenta√ß√£o: http://{API_HOST}:{API_PORT}/docs")
    print(f"ü§ñ Modelo: {OLLAMA_MODEL}")
    print(f"üåê Ollama Host: {OLLAMA_HOST}")
    print(f"üí° Cada requisi√ß√£o √© independente")
    print("="*50 + "\n")
    
    uvicorn.run(
        "main:app",
        host=API_HOST,
        port=API_PORT,
        reload=True
    )