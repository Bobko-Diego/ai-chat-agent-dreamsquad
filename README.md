# AI Chat Agent DreamSquad ü¶Ñüëæ

API de Chat com Agente de IA capaz de realizar c√°lculos matem√°ticos utilizando FastAPI, Strands Agents SDK e Ollama.

## Descri√ß√£o

Este projeto implementa uma API REST que se conecta a um Agente de IA configurado para:

-   Responder perguntas de conhecimento geral
-   Realizar c√°lculos matem√°ticos usando uma tool especializada, quando necess√°rio
-   Identificar automaticamente quando usar ferramentas externas

## Estrutura do Projeto

```
ia-chat-agent-dreamsquad/
‚îú‚îÄ‚îÄ .env                    # Vari√°veis de ambiente
‚îú‚îÄ‚îÄ .gitignore             # Ignorados pelo Git
‚îú‚îÄ‚îÄ README.md              # Documenta√ß√£o
‚îú‚îÄ‚îÄ requirements.txt       # Depend√™ncias
‚îú‚îÄ‚îÄ main.py               # API principal com FastAPI
‚îî‚îÄ‚îÄ tools/
    ‚îî‚îÄ‚îÄ calculator.py     # Tool de c√°lculo matem√°tico
```

## Pr√©-requisitos

Antes de come√ßar, lembre-se de ter instalado:

1. **Python 3.8+**

    ```bash
    python3 --version
    ```

2. **Ollama** (j√° instalado no seu MacBook)

    ```bash
    ollama --version
    ```

3. **Modelo Gemma2** (j√° baixado)
    ```bash
    ollama list
    ```
    Voc√™ deve ver `llama3.2:latest` na lista.

## ‚öôÔ∏è Instala√ß√£o

### 1. Clone ou crie o projeto

```bash
cd ai-chat-agent-dreamsquad
```

### 2. Crie e ative um ambiente virtual

```bash

python3 -m venv venv

# Ativar no macOS/Linux
source venv/bin/activate
```

Quando o ambiente virtual estiver ativado, voc√™ ver√° `(venv)` no in√≠cio do prompt.

### 3. Instale as depend√™ncias

```bash
pip install --upgrade pip
pip install -r requirements.txt
# ou
pip install --upgrade -r requirements.txt
```

### 4. Configure as vari√°veis de ambiente

O arquivo `.env` deve estar configurado com valores padr√£o. Crie um arquivo `.env` e insira as informa√ß√µes abaixo:

```env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:latest
API_HOST=0.0.0.0
API_PORT=8000
AGENT_MAX_TOKENS=2000
AGENT_TEMPERATURE=0.7
```

## Execu√ß√£o

### 1. Certifique-se de que o Ollama est√° rodando

Ao rodar o comando abaixo, √© esperado um JSON tipo '{models: [{...}]}' como retorno

```bash
curl http://localhost:11434/api/tags
```

Se n√£o tiver retornado o JSON, inicie o Ollama:

```bash
ollama serve
```

### 2. Inicie a API

Na primeira janela do terminal rode o comando:

```bash
python main.py
```

Se a API startar corretamente, aparecer√° a mensagem:

```
üöÄ Iniciando Chat Agent API
üìç URL: http://0.0.0.0:8000
üìö Documenta√ß√£o: http://0.0.0.0:8000/docs
ü§ñ Modelo: llama3.2:latest
üåê Ollama Host: http://localhost:11434
üí° Cada requisi√ß√£o √© independente
```

## Endpoints da API

### 1. `GET /` - Informa√ß√µes da API

```bash
curl http://localhost:8000/
```

### 2. `GET /health` - Health Check

```bash
curl http://localhost:8000/health
```

### 3. `POST /chat` - Testando o chat com o Agente

Quando a primeira janela de terminal estiver com a API startada corretamente, abra uma segunda janela de temrinal e cole o seguinte:

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Ol√°"}'
```

**Importante:** Optei pela API √© **stateless**, ent√£o cada requisi√ß√£o √© independente e n√£o mant√©m hist√≥rico de conversas anteriores.

### Teste de c√°lculo 1: C√°lculo simples (usa a tool)

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Quanto √© 2391 * 4295?"}'
```

**Resposta esperada:**

```json
{
    "response": "O resultado √©: 10269345"
}
```

### Teste de c√°lculo 2: Raiz quadrada (usa a tool)

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Qual a raiz quadrada de 144?"}'
```

**Resposta esperada:**

```json
{
    "response": "A raiz quadrada de 144 √© 12"
}
```

### Teste sem c√°lculo 3: Conhecimento geral (n√£o usa a tool)

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Quem foi Albert Einstein?"}'
```

**Resposta esperada:**

```json
{
    "response": "Albert Einstein foi um f√≠sico te√≥rico alem√£o..."
}
```

### Teste 4: Via Interface Swagger

Acesse: http://localhost:8000/docs

Na interface Swagger voc√™ pode:

1. Clicar em `POST /chat`
2. Clicar em "Try it out"
3. Inserir uma mensagem no campo `message`
4. Clicar em "Execute"

## Solu√ß√£o de Problemas

### Problema: "Agente n√£o est√° dispon√≠vel ou algo parecido"

**Solu√ß√£o:** Certifique-se de que o Ollama est√° rodando:

```bash
ollama serve
```

### Problema: "Model not found"

**Solu√ß√£o:** Baixe o modelo usado para o desenvolvimento desse projeto:

```bash
ollama pull llama3.2
```

### Problema: "Connection refused"

**Solu√ß√£o:** Verifique se a porta 11434 do Ollama est√° acess√≠vel:

```bash
curl http://localhost:11434/api/tags
```

### Problema: Importa√ß√£o de m√≥dulos

**Solu√ß√£o:** Certifique-se de que o ambiente virtual est√° ativado:

```bash
source venv/bin/activate
pip install --update -r requirements.txt
```

## Tecnologias Utilizadas

-   **[FastAPI](https://fastapi.tiangolo.com/)**: Framework web moderno e r√°pido para Python
-   **[Strands Agents](https://strandsagents.com/latest/documentation/docs/)**: Framework para cria√ß√£o de agentes de IA
-   **[Ollama](https://ollama.com/)**: Runtime local para modelos de linguagem
-   **[Gemma2]**: Modelo de linguagem do Google
-   **[Python-dotenv](https://pypi.org/project/python-dotenv/)**: Gerenciamento de vari√°veis de ambiente
-   **[Uvicorn]**: Servidor ASGI de alta performance

## Licen√ßa

Este projeto foi desenvolvido como parte de um processo seletivo.

---

Desenvolvido com dedica√ß√£o para o time de #dreamers!
